{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect(\"/Users/andresgf91/Capstone/Data/SQL_TEXAS_HARVEY.sqlite\")\n",
    "\n",
    "test = pd.read_sql_query(\"SELECT CAST(user_id_str as INT) as user_id_str FROM all_texas_tweets limit 100000;\", conn)\n",
    "\n",
    "len(test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pd.DataFrame.to_csv(dates,path_or_buf = \"~/Capstone/Data/raw_dates.csv\", index= False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect(\"/Users/andresgf91/Capstone/Data/SQL_TEXAS_HARVEY.sqlite\")\n",
    "\n",
    "tweet_df = pd.read_sql_query(\"SELECT M.text,CAST(D.id_str as INT) as id_str, M.created_at,\\\n",
    "    CAST(M.user_id_str as INT) as user_id_str, M.name, M.user_lang, M.country_code,M.lang,\\\n",
    "    CAST(M.place_lat as INT)as place_lat, CAST(M.place_lon as INT)as place_lon,\\\n",
    "    CAST(M.lat as INT) as lat, CAST(M.lon as INT) as lon,M.user_county \\\n",
    "    FROM [all_texas_tweets] M JOIN two_year_ids D ON M.id_str=D.id_str LIMIT 10;\", conn)\n",
    "\n",
    "len(tweet_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect(\"/Users/andresgf91/Capstone/Data/SQL_TEXAS_HARVEY.sqlite\")\n",
    "\n",
    "tweet_df = pd.read_sql_query(\"SELECT text,CAST(id_str as INT) as id_str, created_at,\\\n",
    "    CAST(user_id_str as INT) as user_id_str, name, user_lang, country_code,lang,\\\n",
    "    CAST(place_lat as INT)as place_lat, CAST(place_lon as INT)as place_lon,\\\n",
    "    CAST(lat as INT) as lat, CAST(lon as INT) as lon,user_county \\\n",
    "    FROM all_texas_tweets WHERE id_str IN (SELECT id_str FROM two_year_ids);\", conn)\n",
    "\n",
    "len(tweet_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect(\"/Users/andresgf91/Capstone/Data/SQL_TEXAS_HARVEY.sqlite\")\n",
    "\n",
    "tweet_df = pd.read_sql_query(\"SELECT text,CAST(id_str as INT) as id_str, created_at,\\\n",
    "    CAST(user_id_str as INT) as user_id_str, name, user_lang, country_code,lang,\\\n",
    "    CAST(place_lat as INT)as place_lat, CAST(place_lon as INT)as place_lon,\\\n",
    "    CAST(lat as INT) as lat, CAST(lon as INT) as lon,user_county \\\n",
    "    FROM all_texas_tweets;\", conn)\n",
    "\n",
    "len(tweet_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "botometer = pd.read_csv(filepath_or_buffer=\"~/Capstone/Data/botometer_2.csv\")\n",
    "len(botometer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "non_bots = botometer.loc[botometer['english_score'] < np.float(0.5)]\n",
    "non_bots = list(non_bots['ids'])\n",
    "len(non_bots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_from_ls(df,colA,ls):\n",
    "    '''Returns subset of df\n",
    "    keeping only rows where values in colA appear in ls\n",
    "    Assumes df is a pandas DataFrame'''\n",
    "    df = df.loc[df[colA].isin(ls)]\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df = subset_from_ls(tweet_df,'user_id_str',non_bots)\n",
    "len(tweet_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def become_a_pickle(data,file_name):\n",
    "    import pickle\n",
    "    '''Stores data in a pickle file'''\n",
    "    with open(file_name, 'wb') as file:\n",
    "        pickle.dump(data,file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print('Python',type(data),'object has been pickled')\n",
    "    \n",
    "def serve_pickle(file_name):\n",
    "    import pickle\n",
    "    '''Retrieves data stored in a pickle file\n",
    "    returns python object originally saved in pickle file'''\n",
    "    with open(file_name, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "become_a_pickle(data=tweet_df,file_name='/Users/andresgf91/Capstone/Data/tweet_df_temp.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove users whose primary tweeting languague is not english\n",
    "tweet_df = tweet_df.loc[tweet_df['user_lang'] == 'en']\n",
    "\n",
    "#remove tweets that are not in english\n",
    "tweet_df = tweet_df.loc[tweet_df['lang'] == 'en']\n",
    "\n",
    "#reset dataframe index\n",
    "tweet_df = tweet_df.reset_index()\n",
    "len(tweet_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df['created_at'] = pd.to_datetime(tweet_df['created_at']).dt.date\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "change twitter format to regular datetime\n",
    "change_twitter_dateformat(tweet_df)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def cut_time(df, after='2016-08-24', before = '2018-08-25'):\n",
    "    len_origi = len(df)\n",
    "    df = df[(df['created_at'] > after) & (df['created_at'] < before)]\n",
    "    removed_tweets = len_origi - len(df)\n",
    "    print(str(removed_tweets) +\"/\" + str(len_origi) + \" have been removed\")\n",
    "    print('Your new df now has tweets only between ' + after + ' and ' + before)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#subset dataframe to include only tweets from a year before and a year after hurricane harvey\n",
    "tweet_df = cut_time(tweet_df)\n",
    "len(tweet_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df = pd.DataFrame.drop_duplicates(tweet_df,subset='id_str')\n",
    "len(tweet_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"/Users/andresgf91/Capstone/Data/SQL_TEXAS_HARVEY.sqlite\")\n",
    "\n",
    "tweet_df.to_sql('target_tweets', con=conn)\n",
    "#commit changes to database\n",
    "conn.commit()\n",
    "#close the connection to SQL database\n",
    "conn.close()\n",
    "\n",
    "print(\"Target table has been saved to SQL database!!!!!!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
